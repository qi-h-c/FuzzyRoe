# FuzzyRoe
**abstract** Multimodal learning has garnered increasing attention due to its ability to integrate heterogeneous data modalities such as text, image, and audio. While Mixture of Experts (MoE) architectures have emerged as a promising paradigm for enhancing computational efficiency and model scalability, conventional MoE designs suffer from two major limitations: (1) fixed skip rates that lack task adaptivity and (2) inefficient expert selection based on static prior knowledge. These constraints hinder performance in large-scale scenarios with mixed task complexities and diverse semantic patterns, such as medical image classification or heterogeneous text understanding.
To address these issues, we propose an enhanced Routing of Experts (RoE) mechanism that integrates fuzzy control theory to enable adaptive and efficient expert selection. Specifically, we design a sparse fuzzy rule base derived from prior knowledge and expert experience, which is interpolated to form a complete rule library on small datasets. This fuzzy logic-based router dynamically generates routing weights based on task complexity, effectively reducing computational cost without compromising model accuracy. Experiments validate that our Fuzzy methods yield 66.9\% time and 67.2\% data reductions in the Router stage, directly cutting total training time and data by 13.4\%. Consistent Adapter/Finetune data with RoE preserves performance stability. Our code is included in the Supplementary Material.
